---
layout: post
title: "Out of distribution"
date: 2022-02-04 09:00:00 -0000
categories: outlier analysis
excerpt_separator: <!--more-->
---

Deep Learning is a great tool. But, one of its problems is that it’s overly optimistic: 
too high confidences for unknown classes. 
That's a problem in an open world where new classes may be encountered. 
A sense of uncertainty is key. 
Realistic confidences are important for decision making, higher-level reasoning, and human-machine interaction.

We have devised a technique to deal with uncertainty in a deep learning model: 
Equidistant Hyperspherical Prototype Networks. 
The paper was accepted at NeurIPS, at a workshop about uncertainty in decision making.
It is a collaboration with Pascal Mettes from the University of Amsterdam.

In the figure below, on the left, it shows the standard model (onehot). 
The unknown fruit is classified as a pear with a very high confidence (i.e., the blue pointer is long). 
On the right, our proposed method (prototypes). This model says it’s far away from the known classes 
(i.e., the blue pointer is in between two of the known classes). 
The demo shows many more examples. 
You can see that the standard model is shockingly certain about classes that are very different from the known classes. 
Whereas our prototype model can say that an unknown class is in between 2 classes.

The prototypes are almost for free. This model has the same backbone network, 
while requiring less weights for the classifier head. 
It just requires 5 additional lines of Python code for the prototypes and the cosine loss.

<img src="https://gertjanburghouts.github.io/pictures/unknown_fruits.jpg">

<!--more-->

Previous methods to obtain lower confidences for unknown classes are:

Model the ‘other’ category as an extra class
Bayes models: a distribution over the model parameters
(Deep) Ensembles: a number of models in which the mutual differences indicate an uncertainty.

For adding a class ‘other’, you have to be lucky that an unknown class looks more like ‘other’ than a known class. 
Bayes and Ensembles require more compute and memory. For instance, ensembles work well for >= 5 models (see a review by Google). 
Running that many models on a (small, limited) system is in practice inconvenient or sometimes impossible. 
This is problematic for embedded systems, such as SNOW’s compute backpack for SPOT.

A very simple, cheap solution turns out to be possible: prototypes. 
These are representations of the known classes by means of vectors. 
The nice thing is that unknown pictures ‘fall’ in between these prototypes. 
That works especially well if the prototypes are ‘equally’ separated. 
Such ‘equidistant’ prototypes improve uncertainty estimation.

Our NeurIPS paper is joint research with Pascal Mettes from the UvA. 
In the paper, we explore the classification of known and unknown scenes. 
This is relevant for robots who need to understand where they are and whether they know the place.

Let me know if you want to apply it or have any questions!
