---
layout: post
title: "Improved Localization of Objects in Context"
date: 2023-05-07 08:00:00 -0000
categories: language vision zero shot GLIP
excerpt_separator: <!--more-->
---

Our paper got accepted by ICRA's <a href="https://microsoft.github.io/robotics.pretraining.workshop.icra/">Pretraining for Robotics</a>! 

We recognize affordances in images. With this, a robot (e.g. SPOT) can find objects in images, that have a particular use for achieving its goal. 
For instance, opening a door by manipulating the handle. 

<img src="https://gertjanburghouts.github.io/pictures/ICRA_2023_method.jpg">

Our method finds such objects better, by leveraging prior knowledge. 
We improve GLIP, which is a language-vision model to find objects in images. 
We improve its inputs, by diversifying the textual prompts (a door can also be opened by a push bar instead of a handle). 
We also improve its outputs, by verification of the found objects, by checking their context (a handle should be close to the door). 

<img src="https://gertjanburghouts.github.io/pictures/results-doors-dataset.jpg">

A valuable asset for for robots searching for various objects without pretraining! 

Paper at ICRA 2023 (see Publications). Funded by TNO Appl.AI program, SNOW project.
